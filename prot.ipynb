{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2db80f11-7487-4ec6-bc08-2059669c5bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\n",
      "ERROR: No matching distribution found for faiss\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61e81ec-b155-458c-835b-8785e77461d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df50ffe-2e51-4eb2-ab5d-3597c17acdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"Парсит строку, извлекает из нее вызовы инструментов.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_tools(text: str):\n",
    "        \"\"\"Парсинг вызовов с выделением инструментов.\n",
    "\n",
    "        Args:\n",
    "            text: список запросов.\n",
    "\n",
    "        Returns:\n",
    "            список из словарей: \n",
    "            'tool' - название инструмента\n",
    "            'request' - запрос, для которого нужно использовать инструмент.\n",
    "        \"\"\"\n",
    "        \n",
    "        reg = r'\\[(.*?)\\]\\s*(.*)'\n",
    "        matches = re.findall(reg, text)\n",
    "        return [{'tool': match[0], 'request': match[1]} for match in matches]\n",
    "\n",
    "class FindTool:\n",
    "    \"\"\"Инструмент [find], реализующий семантический поиск по базе знаний\"\"\"\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"Args:\n",
    "            path: путь к директории.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        directory_path : str\n",
    "            путь к директории\n",
    "        documents : list\n",
    "            полученные из директории документы\n",
    "        texts : list\n",
    "            извлеченный из документов текст\n",
    "        \"\"\"\n",
    "        \n",
    "        self.directory_path = path\n",
    "        self.documents = []\n",
    "        self.texts = []\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.word_vectors = None\n",
    "        self.word2vec_model = None\n",
    "\n",
    "    def load_documents(self):\n",
    "        \"\"\"Загружает файлы из директории.\"\"\"\n",
    "        \n",
    "        for root, _, files in os.walk(self.directory_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                text = self.text_extraction(file_path)\n",
    "                if text:\n",
    "                    self.documents.append({\n",
    "                        'path': file_path,\n",
    "                        'text': text\n",
    "                    })\n",
    "                    self.texts.append(text)\n",
    "\n",
    "    def text_extraction(self, path: str) -> str:\n",
    "        \"\"\"Извлекает текст из файлов с различными разрешениями (pdf, docx, csv, xlsx, txt).\n",
    "\n",
    "        Args:\n",
    "            path: путь к директории\n",
    "\n",
    "        Returns:\n",
    "            текст, полученный из документа.\n",
    "        \"\"\"\n",
    "        \n",
    "        if path.endswith('.pdf'):\n",
    "            with open(path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                text = '\\n'.join([page.extract_text() for page in reader.pages])\n",
    "        elif path.endswith('.docx'):\n",
    "            text = docx2txt.process(path)\n",
    "        elif path.endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "            text = \" \".join(df.to_string().split())\n",
    "        elif path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(path)\n",
    "            text = \" \".join(df.to_string().split())\n",
    "        else:\n",
    "            with open(path, 'r', encoding='utf-8') as f: \n",
    "                text = f.read() #обрабатывает текстовые форматы (.txt)\n",
    "        return text\n",
    "\n",
    "    def preprocessing(self, text: str) -> list:\n",
    "        \"\"\"Токенизация текста: удаление знаков препинания, вспомогательных слов, разбиение\"\"\"\n",
    "        \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def build_tfidf_model(self):\n",
    "        \"\"\"Построение модели TF-IDF, выделение ключевых слов\"\"\"\n",
    "        \n",
    "        processed = [\" \".join(self.preprocessing(text)) for text in self.texts]\n",
    "        self.vectorizer.fit(processed)\n",
    "\n",
    "    def build_word2vec_model(self):\n",
    "        \"\"\"Построение модели Word2Vec для поиска семантически похожих слов\"\"\"\n",
    "        \n",
    "        sentences = [self.preprocessing(text) for text in self.texts]\n",
    "        self.word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    def semantic_search(self, word: str):\n",
    "        \"\"\"Осуществляет поиск семантически похожих на заданное слов в тексте.\n",
    "\n",
    "        Args:\n",
    "            word: искомое слово\n",
    "\n",
    "        Returns:\n",
    "            список семантически похожих слов. \n",
    "            в случае отсутствия совпадений выводит соответствующее уведомление.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        word_tokens = self.preprocessing(word)\n",
    "        similar_words = []\n",
    "        \n",
    "        for token in word_tokens:\n",
    "            if token in self.word2vec_model.wv:\n",
    "                similar = self.word2vec_model.wv.most_similar(token, topn=n)\n",
    "                similar_words.extend([(wrd, score) for wrd, score in similar])\n",
    "        \n",
    "        similar_words = list(set(similar_words))\n",
    "        similar_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if len(similar_words) > 0:\n",
    "            return [similar_words[i][0] for i in range(0, len(similar_words))]\n",
    "        return \"No matches found\"\n",
    "\n",
    "class NeuroResponse: \n",
    "    \"\"\"Обрабатывает запросы нейросети, использует инструменты.\"\"\"\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"Args:\n",
    "            path: путь к директории.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        search : FindTool class object\n",
    "            аттрибут для семантического поиска.\n",
    "        parser : Parser class object\n",
    "            аттрибут для парсинга запросов.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.search = FindTool(path)\n",
    "        self.parser = Parser()\n",
    "\n",
    "    def response_process(self, text: str):\n",
    "        \"\"\"Непосредственно обрабатывает запросы.\n",
    "\n",
    "        Args:\n",
    "            text: текст запроса\n",
    "\n",
    "        Returns:\n",
    "            результат использования инструментов из запросов.\n",
    "        \"\"\"\n",
    "        \n",
    "        requests = self.parser.parse_tools(text)\n",
    "        self.search.load_documents()\n",
    "        self.search.build_word2vec_model()\n",
    "        result = []\n",
    "\n",
    "        for req in requests: \n",
    "            if req['tool'] == 'find':\n",
    "                search_results = self.search.semantic_search(req['request'])\n",
    "                print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fea68a64-faad-4d63-86ab-ba1ae2ff2c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gimmemore\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
